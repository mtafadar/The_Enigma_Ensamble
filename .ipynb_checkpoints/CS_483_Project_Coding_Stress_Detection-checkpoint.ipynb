{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlaW5KPFIdsC"
   },
   "source": [
    "## <em>A Big Data Mining Approach Project</em>\n",
    "## <b>Stress Detecting from Social Media Interaction</b>\n",
    "## Group name: The Enigma Ensemble\n",
    "\n",
    "### <em>(*) First author:</em>\n",
    "##### Tri Quan Do (tdo22@uic.edu) - Group Leader\n",
    "##### Mosrour Tafadar (mtafad2@uic.edu)\n",
    "##### Hina Khali (hkhali21@uic.edu)\n",
    "##### Safiya Mustafa (smust3@uic.edu)\n",
    "\n",
    "\n",
    "## Project Abstract:\n",
    "Emotional and mental stress are serious issues that can have a significant impact on our well-being. Despite the fact that an emotional experience usually starts as a personal, internal process, it frequently results in the communal sharing of emotions with others. Emotions that are verbally expressed to others by the individual who has experienced them are referred to as being socially shared. People share their emotions with others in more than 80% of all emotional events, regardless of their age, gender, personality type, or culture (Bazarova, Choi, Sosik, Cosley, Whitlock 1). Due to social media's widespread use, people are accustomed to posting about their everyday activities and connecting with acquaintances on these platforms, making it possible to use information from online social networks to identify stress.\n",
    "\n",
    "## Project Introduction\n",
    "\n",
    "The initial step of this research project involves identifying a set of words that are commonly associated with emotional stress. Using this set of words, the models aim to compute an overall stress score for each individual under investigation. However, it is critical to acknowledge that some words may carry a higher intensity than others. Hence, the project purpose will segregate the identified set of words into distinct categories based on their intensity levels, namely high, moderate, and low to parallel conduct a word frequency analysis to identify words or phrases that occur frequently, specifically those that pertain to emotions or stress. This research approach is expected to provide valuable insights into the underlying patterns and associations between language use and emotional stress, thereby contributing to the existing knowledge base on the topic.<br><br>\n",
    "\n",
    "Robust technologies for processing and analyzing massive amounts of social media data include Support Vector Machines (SVM) and MapReduce, which can be used to forecast stress levels based on social media posts. SVM is a machine learning algorithm that divides the data into classes before identifying the hyperplane that best distinguishes the classes. Large datasets can be processed concurrently on a distributed computing system using the model and software framework known as MapReduce\n",
    "\n",
    "Full project information could be found here <\"add link to document\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#######################################################\n",
    "###########   ENVIRONMENT SETTING UP   ################\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install -U scikit-learn\n",
    "!pip install seaborn\n",
    "!pip install pyspark\n",
    "!pip install -U -q PyDrive\n",
    "!apt install openjdk-8-jdk-headless -qq"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXsy0Z5MJ39m"
   },
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "## These code below could generate error when working on non-google colab environment    ##\n",
    "## Please comment those code below if you work on local machine                          ##\n",
    "###########################################################################################\n",
    "\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "#\n",
    "# # Authenticate and create the PyDrive client\n",
    "# auth.authenticate_user()\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = GoogleCredentials.get_application_default()\n",
    "# drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjsoAutlJ8Nh"
   },
   "outputs": [],
   "source": [
    "from oauth2client.crypt import PyCryptoSigner\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Modeling for Machine Learning Task\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGW5OPAZccM4"
   },
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yjb3mj5dchZW"
   },
   "source": [
    "## Data Cleaning Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_many(file_path=\"\", expect_col=[], file_list=[], encode=\"\", read_many=False):\n",
    "  \"\"\"\n",
    "  Reads and returns multiple CSV files as pandas dataframes.\n",
    "\n",
    "  Args:\n",
    "      file_path (str): Optional file path to read CSV files from.\n",
    "      expect_col (list): Optional list of expected column names to extract from each CSV file.\n",
    "      file_list (list): List of CSV file names to read.\n",
    "      encode (str): Optional encoding type for reading CSV files.\n",
    "      read_many (bool): Optional boolean to indicate whether to read multiple CSV files.\n",
    "\n",
    "  Returns:\n",
    "      list: A list of pandas dataframes, where each dataframe corresponds to a CSV file in file_list.\n",
    "  \"\"\"\n",
    "\n",
    "  dataFrame_list = []\n",
    "\n",
    "  for f_name in file_list:\n",
    "    data_csv = pd.read_csv(f_name)\n",
    "    dataFrame_list.append(data_csv)\n",
    "\n",
    "  dataFrames_Final = []\n",
    "  # Drop unnecessary columns that only retrieve from expected one\n",
    "  if len(expect_col) > 0:\n",
    "    for frame in dataFrame_list:\n",
    "      new_frame = frame.loc[:, expect_col]\n",
    "      dataFrames_Final.append(new_frame)\n",
    "  else:\n",
    "    dataFrames_Final = dataFrame_list\n",
    "\n",
    "  return dataFrames_Final"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_one(file_path=\"\", expect_col=[], encode=\"\", drop_NaN=False):\n",
    "  \"\"\"\n",
    "  Reads a single CSV file as a pandas dataframe, drops NaN rows and columns, and returns the resulting dataframe.\n",
    "\n",
    "  Args:\n",
    "      file_path (str): Optional file path to read the CSV file from.\n",
    "      expect_col (list): Optional list of expected column names to extract from the CSV file.\n",
    "      encode (str): Optional encoding type for reading the CSV file.\n",
    "      drop_NaN (bool): Optional boolean to indicate whether to drop NaN rows and columns.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: A pandas dataframe that corresponds to the CSV file in file_path, after cleaning.\n",
    "\n",
    "  Notes:\n",
    "      If drop_NaN is True, rows and columns with NaN values will be dropped. If expect_col is non-empty,\n",
    "      only the specified columns will be retained. If both options are used, NaN rows and columns will be\n",
    "      dropped first, and then the specified columns will be retained.\n",
    "  \"\"\"\n",
    "\n",
    "  # Case when import 1 single file only\n",
    "  data_csv = pd.read_csv(file_path, encoding=encode)\n",
    "\n",
    "  # Drop un-clean data or data row incomplete\n",
    "  if drop_NaN:\n",
    "    data_csv.dropna(inplace=True)           # drop rows missed value\n",
    "    data_csv.to_csv(\"twitter_content_wb.csv\", index=False) # Write back\n",
    "\n",
    "  # Drop unnecessary columns that only retrieve from expected one\n",
    "  if len(expect_col) > 0:\n",
    "    data_csv = data_csv.loc[:, expect_col]\n",
    "\n",
    "  return data_csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Clean the data to extract expected columns\n",
    "def data_import(file_path=\"\", expect_col=[], file_list=[], encode=\"\", read_many=False):\n",
    "  \"\"\"\n",
    "    This function imports crime data from a CSV file or\n",
    "    a list of CSV files. It drops missing values and\n",
    "    unnecessary columns from the data and returns\n",
    "    a Pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): the path to the CSV file to import\n",
    "      (default: \"\")\n",
    "    expect_col (list): a list of column names to keep in the data\n",
    "      (default: [])\n",
    "    file_list (list): a list of file paths to import if read_many is True\n",
    "      (default: [])\n",
    "    read_many (bool): True if importing multiple files, False if importing a single file\n",
    "      (default: False)\n",
    "\n",
    "    Returns:\n",
    "    A Pandas DataFrame containing the cleaned crime data.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    # When reading multiples file, return a list of frames\n",
    "    if read_many is True:\n",
    "      return read_many(file_path, expect_col, file_list, encode, read_many)\n",
    "\n",
    "    # Case when import 1 single file only\n",
    "    return read_one(file_path, expect_col, encode)\n",
    "\n",
    "  # Internal error occurred\n",
    "  except Exception as e:\n",
    "    try:\n",
    "      # Case when import 1 single file only\n",
    "      return read_one(file_path, expect_col, encode)\n",
    "    except Exception as e:\n",
    "      print(\"Internal errors occurs for loading csv file. Try again\", str(e))\n",
    "      return None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data file storage - user can change CONST_PATH to his/her location\n",
    "CONST_PATHDIR = \"Training Data/twitter_content.csv\"\n",
    "CONST_ENCODES = 'ISO-8859-1'\n",
    "signi_columns = ['_unit_id', 'gender', 'created', 'description', 'name', 'retweet_count','text']\n",
    "twitter_Frame = data_import(file_path=CONST_PATHDIR, expect_col=signi_columns, encode=CONST_ENCODES)\n",
    "twitter_Frame.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "twitter_Frame.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
